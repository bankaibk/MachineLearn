{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abc4fa6-7186-470a-84f7-d4d0f39ecdc3",
   "metadata": {},
   "source": [
    "## 3.无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fac16-5d46-44f1-8224-d8f9c0f9669b",
   "metadata": {},
   "source": [
    "### 3. 1PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff53d0-9f94-411b-94fa-a37a652fa038",
   "metadata": {},
   "source": [
    "PCA 是一种用于减少数据中的变量的算法。它对变量之间存在相关性的数据很有效，是一种具有代表性的降维算法；<br>\n",
    "PCA 基于原有的变量构造新的变量；<br>\n",
    "PCA 可发现对象数据的方向和重要度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4046c99-0e80-4be7-a0eb-67104dfa41f0",
   "metadata": {},
   "source": [
    "(a)图中画出正交的两条线，线的方向表示数据的方向，长度表示重要度。方向由构成新变量时对象数据变量的权重决定，而重要度与变量的偏差有关。<br>\n",
    "(b)图以这两条线为新轴对原始数据进行变换后得到的图形，变换后的数据称为主成分得分。按主成分轴的重要度的值从高到低排序，依次称它们为第一主成分、第二主成分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd29c6-258f-45d8-b739-555b5ba130e9",
   "metadata": {},
   "source": [
    "![title](./image/PCA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b32cbd-b80f-4e8e-8102-7382e13b2d81",
   "metadata": {},
   "source": [
    "接下来介绍PCA 寻找主成分的步骤\n",
    "1. 计算协方差矩阵。\n",
    "2. 对协方差矩阵求解特征值问题，求出特征向量和特征值。\n",
    "3. 以数据表示各主成分方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec094ef4-69f7-4230-985d-96e3c092ec9f",
   "metadata": {},
   "source": [
    "下面是对 scikit-learn 内置的鸢尾花数据使用 PCA 的示例代码，用于将 4 个特征变量变换为 2个主成分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a98f300-fbff-44e4-b367-0951f4872579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.68412563  0.31939725]\n",
      " [-2.71414169 -0.17700123]\n",
      " [-2.88899057 -0.14494943]\n",
      " [-2.74534286 -0.31829898]\n",
      " [-2.72871654  0.32675451]\n",
      " [-2.28085963  0.74133045]\n",
      " [-2.82053775 -0.08946138]\n",
      " [-2.62614497  0.16338496]\n",
      " [-2.88638273 -0.57831175]\n",
      " [-2.6727558  -0.11377425]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "n_components = 2 #将减少后的维度设置为2\n",
    "model = PCA(n_components=n_components)\n",
    "model = model.fit(data.data)\n",
    "print(model.transform(data.data)[:10]) # 变换后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016951f-0f1c-4bab-88c5-03aace4a4926",
   "metadata": {},
   "source": [
    "PCA 可以将原始数据中的变量表示为新的轴的主成分。这些主成分按照贡献率大小排序，分别\n",
    "为第一主成分、第二主成分……照此类推。这时通过计算累计贡献率，我们可以知道使用到第几个\n",
    "主成分为止可以包含原始数据多少比例的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318eb94c-c5ab-4b61-87a3-db5a34e9564a",
   "metadata": {},
   "source": [
    "### 3.2 LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c409f3-7833-4a9b-9c35-81b023e6d757",
   "metadata": {},
   "source": [
    "使用 LSA 就可以根据大量文本自动计算单词和单词的相似度，以及单词和文本的相似度。\n",
    "通过 LSA 对文本和单词的矩阵进行降维，将其变换为潜在语义空间（图 3-5）。这种变换使用\n",
    "矩阵分解进行。矩阵分解是指将某个矩阵表示为多个矩阵的乘积的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ab2a1-5025-4995-9baa-c274ed30904b",
   "metadata": {},
   "source": [
    "下面结合具体的例子来讲解矩阵分解和降维。\n",
    "首先将以下文本变换为矩阵 X。矩阵 X 的各元素是文本中出现的单词的个数\n",
    "- 坐汽车去公司\n",
    "- 坐车去的\n",
    "- 在餐厅吃汉堡牛肉饼\n",
    "- 在餐厅吃意大利面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e89ea-bda0-486d-a194-5d6abd912772",
   "metadata": {},
   "source": [
    "![title](./image/wordNum.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2b7f5-66b5-45c1-a5ad-d735bf3a2639",
   "metadata": {},
   "source": [
    "下一步：通过矩阵变换进行矩阵分解和降维，将4维的特征转换为2维<br>\n",
    "![title](./image/wordNumChange.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fc6ad-e719-4c36-bbad-a7af51c37d1f",
   "metadata": {},
   "source": [
    "此时得到的数据可以这样理解：<br>\n",
    "“汽车”和“车”拥有变量 B 的值，<br>\n",
    "“汉堡牛肉饼”和“意大利面”拥有变量 A 的值。<br>\n",
    "A 和 B 的特征值显示了各个单词之间的关联性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fd79f-28c6-4413-9714-eff1c0efdd6d",
   "metadata": {},
   "source": [
    "下面使用 Python 代码解决前面探讨的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe03c45-405b-44df-97db-a34535f5b5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  8.50650808e-01]\n",
      " [ 0.00000000e+00  8.50650808e-01]\n",
      " [ 2.71947991e-16  1.37638192e+00]\n",
      " [ 2.71947991e-16  5.25731112e-01]\n",
      " [ 1.41421356e+00 -2.02192262e-16]\n",
      " [ 7.07106781e-01  0.00000000e+00]\n",
      " [ 1.41421356e+00 -2.02192262e-16]\n",
      " [ 7.07106781e-01 -2.02192262e-16]]\n",
      "[0.38596491 0.27999429]\n",
      "0.6659592065833293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "data = [[1, 0, 0, 0],\n",
    "[1, 0, 0, 0],\n",
    "[1, 1, 0, 0],\n",
    "[0, 1, 0, 0],\n",
    "[0, 0, 1, 1],\n",
    "[0, 0, 1, 0],\n",
    "[0, 0, 1, 1],\n",
    "[0, 0, 0, 1]]\n",
    "n_components = 2 # 潜在变量的个数\n",
    "model = TruncatedSVD(n_components=n_components)\n",
    "model.fit(data)\n",
    "print(model.transform(data)) # 变换后的数据\n",
    "print(model.explained_variance_ratio_) # 贡献率\n",
    "print(sum(model.explained_variance_ratio_)) # 累计贡献率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efc0c5-1709-4bb5-99a1-2cc02df485ae",
   "metadata": {},
   "source": [
    "LSA算法的注意事项\n",
    "1. 变换后的矩阵有时难以解释。在通过奇异值分解降维时，各个维度可能是正交的，矩阵中的元素也可能是负值。\n",
    "2. LSA 的计算成本有时很高。特别是在用于文本时，由于原始矩阵的维度就是单词的个数，所以 LSA 必须在非常大的矩阵上进行奇异值分解。\n",
    "3. 随着新词的加入，原有的矩阵必须重新创建，我们必须在此基础上重新计算，所以模型的更新难度很大。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
