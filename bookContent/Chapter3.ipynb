{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abc4fa6-7186-470a-84f7-d4d0f39ecdc3",
   "metadata": {},
   "source": [
    "## 3.无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fac16-5d46-44f1-8224-d8f9c0f9669b",
   "metadata": {},
   "source": [
    "### 3. 1PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff53d0-9f94-411b-94fa-a37a652fa038",
   "metadata": {},
   "source": [
    "PCA 是一种用于减少数据中的变量的算法。它对变量之间存在相关性的数据很有效，是一种具有代表性的降维算法；<br>\n",
    "PCA 基于原有的变量构造新的变量；<br>\n",
    "PCA 可发现对象数据的方向和重要度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4046c99-0e80-4be7-a0eb-67104dfa41f0",
   "metadata": {},
   "source": [
    "(a)图中画出正交的两条线，线的方向表示数据的方向，长度表示重要度。方向由构成新变量时对象数据变量的权重决定，而重要度与变量的偏差有关。<br>\n",
    "(b)图以这两条线为新轴对原始数据进行变换后得到的图形，变换后的数据称为主成分得分。按主成分轴的重要度的值从高到低排序，依次称它们为第一主成分、第二主成分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd29c6-258f-45d8-b739-555b5ba130e9",
   "metadata": {},
   "source": [
    "![title](./image/PCA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b32cbd-b80f-4e8e-8102-7382e13b2d81",
   "metadata": {},
   "source": [
    "接下来介绍PCA 寻找主成分的步骤\n",
    "1. 计算协方差矩阵。\n",
    "2. 对协方差矩阵求解特征值问题，求出特征向量和特征值。\n",
    "3. 以数据表示各主成分方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec094ef4-69f7-4230-985d-96e3c092ec9f",
   "metadata": {},
   "source": [
    "下面是对 scikit-learn 内置的鸢尾花数据使用 PCA 的示例代码，用于将 4 个特征变量变换为 2个主成分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a98f300-fbff-44e4-b367-0951f4872579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.68412563  0.31939725]\n",
      " [-2.71414169 -0.17700123]\n",
      " [-2.88899057 -0.14494943]\n",
      " [-2.74534286 -0.31829898]\n",
      " [-2.72871654  0.32675451]\n",
      " [-2.28085963  0.74133045]\n",
      " [-2.82053775 -0.08946138]\n",
      " [-2.62614497  0.16338496]\n",
      " [-2.88638273 -0.57831175]\n",
      " [-2.6727558  -0.11377425]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "n_components = 2 #将减少后的维度设置为2\n",
    "model = PCA(n_components=n_components)\n",
    "model = model.fit(data.data)\n",
    "print(model.transform(data.data)[:10]) # 变换后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f016951f-0f1c-4bab-88c5-03aace4a4926",
   "metadata": {},
   "source": [
    "PCA 可以将原始数据中的变量表示为新的轴的主成分。这些主成分按照贡献率大小排序，分别\n",
    "为第一主成分、第二主成分……照此类推。这时通过计算累计贡献率，我们可以知道使用到第几个\n",
    "主成分为止可以包含原始数据多少比例的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318eb94c-c5ab-4b61-87a3-db5a34e9564a",
   "metadata": {},
   "source": [
    "### 3.2 LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c409f3-7833-4a9b-9c35-81b023e6d757",
   "metadata": {},
   "source": [
    "使用 LSA 就可以根据大量文本自动计算单词和单词的相似度，以及单词和文本的相似度。\n",
    "通过 LSA 对文本和单词的矩阵进行降维，将其变换为潜在语义空间（图 3-5）。这种变换使用\n",
    "矩阵分解进行。矩阵分解是指将某个矩阵表示为多个矩阵的乘积的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ab2a1-5025-4995-9baa-c274ed30904b",
   "metadata": {},
   "source": [
    "下面结合具体的例子来讲解矩阵分解和降维。\n",
    "首先将以下文本变换为矩阵 X。矩阵 X 的各元素是文本中出现的单词的个数\n",
    "- 坐汽车去公司\n",
    "- 坐车去的\n",
    "- 在餐厅吃汉堡牛肉饼\n",
    "- 在餐厅吃意大利面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e89ea-bda0-486d-a194-5d6abd912772",
   "metadata": {},
   "source": [
    "![title](./image/wordNum.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2b7f5-66b5-45c1-a5ad-d735bf3a2639",
   "metadata": {},
   "source": [
    "下一步：通过矩阵变换进行矩阵分解和降维，将4维的特征转换为2维<br>\n",
    "![title](./image/wordNumChange.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fc6ad-e719-4c36-bbad-a7af51c37d1f",
   "metadata": {},
   "source": [
    "此时得到的数据可以这样理解：<br>\n",
    "“汽车”和“车”拥有变量 B 的值，<br>\n",
    "“汉堡牛肉饼”和“意大利面”拥有变量 A 的值。<br>\n",
    "A 和 B 的特征值显示了各个单词之间的关联性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fd79f-28c6-4413-9714-eff1c0efdd6d",
   "metadata": {},
   "source": [
    "下面使用 Python 代码解决前面探讨的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "600ee0be-af98-4bc6-92f2-6f839807b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  8.50650808e-01]\n",
      " [ 0.00000000e+00  8.50650808e-01]\n",
      " [ 2.71947991e-16  1.37638192e+00]\n",
      " [ 2.71947991e-16  5.25731112e-01]\n",
      " [ 1.41421356e+00 -2.02192262e-16]\n",
      " [ 7.07106781e-01 -2.02192262e-16]\n",
      " [ 1.41421356e+00 -2.02192262e-16]\n",
      " [ 7.07106781e-01  0.00000000e+00]]\n",
      "[0.38596491 0.27999429]\n",
      "0.6659592065833293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "data = [[1, 0, 0, 0],\n",
    "[1, 0, 0, 0],\n",
    "[1, 1, 0, 0],\n",
    "[0, 1, 0, 0],\n",
    "[0, 0, 1, 1],\n",
    "[0, 0, 1, 0],\n",
    "[0, 0, 1, 1],\n",
    "[0, 0, 0, 1]]\n",
    "n_components = 2 # 潜在变量的个数\n",
    "model = TruncatedSVD(n_components=n_components)\n",
    "model.fit(data)\n",
    "print(model.transform(data)) # 变换后的数据\n",
    "print(model.explained_variance_ratio_) # 贡献率\n",
    "print(sum(model.explained_variance_ratio_)) # 累计贡献率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efc0c5-1709-4bb5-99a1-2cc02df485ae",
   "metadata": {},
   "source": [
    "LSA算法的注意事项\n",
    "1. 变换后的矩阵有时难以解释。在通过奇异值分解降维时，各个维度可能是正交的，矩阵中的元素也可能是负值。\n",
    "2. LSA 的计算成本有时很高。特别是在用于文本时，由于原始矩阵的维度就是单词的个数，所以 LSA 必须在非常大的矩阵上进行奇异值分解。\n",
    "3. 随着新词的加入，原有的矩阵必须重新创建，我们必须在此基础上重新计算，所以模型的更新难度很大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15217e70-7212-47e2-83e7-82b5a32658ff",
   "metadata": {},
   "source": [
    "### 3.3NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6f1d0-c977-4c64-9618-7951995a5825",
   "metadata": {},
   "source": [
    "NMF 是一种矩阵分解方法，在计算机视觉、文本挖掘、推荐等各个领域都有应用。与LSA一样，它也可以找到矩阵的潜在变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09511c07-915a-42bc-8fdb-6880084c0a24",
   "metadata": {},
   "source": [
    "NMF具有一下特点\n",
    "- 原始矩阵的元素是非负数。\n",
    "- 分解后矩阵的元素是非负数。\n",
    "- 没有“潜在语义空间的每一个维度都是正交的”这一约束条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d22531-133f-4598-844f-5459f9a14711",
   "metadata": {},
   "source": [
    "算法说明</br>\n",
    "NMF 作为一种矩阵分解方法，通过将原始数据分解为两个矩阵来降维。设原始数据为 n 行 d 列的矩阵 V，将其表示为两个矩阵 W 和 H 的乘积。W 是 n 行 r 列的矩阵，H 是 r 行 d 列的矩阵，WH 是原始矩阵 V 的近似，选择比 d 小的 r 就可以进行降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e1598-4cc0-4ae6-aa23-dd677a05b293",
   "metadata": {},
   "source": [
    "![title](./image/NMF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea44d18-dc19-4cf4-8f54-01ff9d965486",
   "metadata": {},
   "source": [
    "在求 W 和 H 的过程中，NMF 在 W ≥ 0、H ≥ 0 的条件下，使 WH 接近 V。\n",
    "NMF 采取“将 H 视为常数，更新 W ”“将 W 视为常数，更新 H”的方式交替更新 W 和 H。</br>\n",
    "灰色的点为原始矩阵 V，绿色的点为近似矩阵 WH。随着计算的进行，我们可以看到近似矩阵\n",
    "越来越接近原始矩阵。此外，红线和蓝线是潜在空间的轴，所有近似矩阵的图形都能在潜在空间的轴上表示出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c6b4c-b805-4c79-a386-a09766a9e418",
   "metadata": {},
   "source": [
    "![title](./image/NMFupdate.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ceeba-ca85-463b-99c4-6497a9e0f13e",
   "metadata": {},
   "source": [
    "下面是使用 scikit-learn 运行 NMF 算法的示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a009e24a-91c4-49bd-8a95-8afd9346e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57375552 0.63740946]\n",
      " [0.16462067 1.01137901]\n",
      " [0.24396182 1.09817687]\n",
      " [0.84111607 0.21069069]\n",
      " [0.76569886 0.27585125]\n",
      " [0.74131722 0.50097646]\n",
      " [0.62518984 0.57436808]\n",
      " [0.25472553 0.92742861]\n",
      " [0.76277361 0.39397145]\n",
      " [0.57470649 0.55103464]]\n",
      "[[ 3.70702256 11.72794881  4.80640961]\n",
      " [ 7.84046896  1.93365196  8.4614337 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets._samples_generator import make_blobs\n",
    "centers = [[5, 10, 5], [10, 4, 10], [6, 8, 8]]\n",
    "V, _ = make_blobs(centers=centers)\n",
    "n_components = 2\n",
    "# 潜在变量的个数\n",
    "# 以centers为中心生成数据\n",
    "model = NMF(n_components=n_components)\n",
    "model.fit(V)\n",
    "W = model.transform(V) # 分解后的矩阵\n",
    "H = model.components_\n",
    "print(W[:10])\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf2fb9-5c4f-47bf-abe2-3cd1c367fcd0",
   "metadata": {},
   "source": [
    "### 3.4 LDA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40802ebe-4ac2-43e4-a075-5bbda9602c0d",
   "metadata": {},
   "source": [
    "LDA 是一种用于自然语言处理等的算法。该算法可以根据文本中的单词找出潜在的主题，并描述每个文本是由什么主题组成的，还可以用于说明一个文本不只有一个主题，而是有多个主题。例如，一篇真实的新闻文本可能包含多个主题，如“体育”和“教育”等，使用 LDA 就可以很好地描述这种新闻文本。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71d8c8d0-6efc-416c-a139-a511368f7630",
   "metadata": {},
   "source": [
    "我们用一个具体的例子进行解释，假设例句中的主题个数为2个\n",
    "1. We go to school on weekdays.\n",
    "2. I like playing sports.\n",
    "3. They enjoyed playing sports in school.\n",
    "4. Did she go there after school?\n",
    "5. He read the sports columns yesterday.\n",
    "LDA 可以利用主题分布和单词分布创建文本数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7431fe-470c-48ba-805d-b467fc472758",
   "metadata": {},
   "source": [
    "![title](./image/LDA.jpg)</br>\n",
    "具体做法是基于文本的主题分布选择主题，之后基于主题的单词分布选择文本中的单词。重复\n",
    "这一操作，就能得到生成文本的模型。下面的“算法说明”部分将介绍如何根据输入数据计算主题\n",
    "分布和单词分布。\n",
    "1. 基于文本的主题分布为单词分配主题。\n",
    "2. 基于分配的主题的单词分布确定单词。\n",
    "3. 对所有文本中包含的单词执行步骤 1 和步骤 2 的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044b1ce-5f71-4429-9369-ab2f2e86fde4",
   "metadata": {},
   "source": [
    "算法说明\n",
    "1. 为各文本的单词随机分配主题。\n",
    "2. 基于为单词分配的主题，计算每个文本的主题概率。\n",
    "3. 基于为单词分配的主题，计算每个主题的单词概率。\n",
    "4. 计算步骤 2 和步骤 3 中的概率的乘积，基于得到的概率，再次为各文本的单词分配主题。\n",
    "5. 重复步骤 2 ~ 步骤 4 的计算，直到收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d394a-d9f7-43da-8eb5-75c6ded06d3f",
   "metadata": {},
   "source": [
    "### 3.5 k -means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef979a02-9f1b-421d-8358-226171fb49ba",
   "metadata": {},
   "source": [
    "k-means 算法是一种有代表性的聚类算法。由于该算法简单易懂，又可以用于比较大的数据集，\n",
    "所以在市场分析和计算机视觉等领域得到了广泛的应用。</br>\n",
    "通过计算数据点与各重心的距离，找出离得最近的簇的重心，可以确定数据点所属的簇。求簇\n",
    "的重心是 k-means 算法中重要的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99797889-a4e5-4cd1-a1b2-5e173eb3b9a4",
   "metadata": {},
   "source": [
    "k-means 算法的典型计算步骤如下\n",
    "1. 从数据点中随机选择数量与簇的数量相同的数据点，作为这些簇的重心。\n",
    "2. 计算数据点与各重心之间的距离，并将最近的重心所在的簇作为该数据点所属的簇。\n",
    "3. 计算每个簇的数据点的平均值，并将其作为新的重心。\n",
    "4. 重复步骤 2 和步骤 3，继续计算，直到所有数据点不改变所属的簇，或者达到最大计算步数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24875ccc-9ecb-4d2d-a1ea-5bbc653dadbf",
   "metadata": {},
   "source": [
    "下面是对鸢尾花数据集应用 k-means 算法的代码，簇的数量设置为 3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbed6e46-5dcf-48cf-bd8b-4ca51f58466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "[[5.9016129  2.7483871  4.39354839 1.43387097]\n",
      " [5.006      3.428      1.462      0.246     ]\n",
      " [6.85       3.07368421 5.74210526 2.07105263]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "n_clusters = 3\n",
    "# 将簇的数量设置为3\n",
    "model = KMeans(n_clusters=n_clusters)\n",
    "model.fit(data.data)\n",
    "print(model.labels_)\n",
    "# 各数据点所属的簇\n",
    "print(model.cluster_centers_)\n",
    "# 通过fit()计算得到的重心"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7f3c9-6240-43f3-a3c1-e11ff69b6dde",
   "metadata": {},
   "source": [
    "### 3.6 混合高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fc0fb-12c5-4411-8547-948dd12dfcfe",
   "metadata": {},
   "source": [
    "高斯分布是混合高斯分布的基础，是统计学和机器学习中经常使用的一种概率分布。数据的分\n",
    "布可以用均值和方差表示，均值描述数据的中心位置，方差描述数据的离散程度。混合高斯分布是\n",
    "以多个高斯分布的线性叠加来表示数据的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd71c9-2850-4879-a9a2-cdada54859d7",
   "metadata": {},
   "source": [
    "算法说明</br>\n",
    "混合高斯分布的学习过程是从给定的数据点中找到每个高斯分布的均值和方差的过程。作为例子，我们采用一维数据。</br>\n",
    "![title](./image/gauss.jpg)</br>\n",
    "红色的点是从均值为 - 2.0、方差为 2.2 的高斯分布中采样得到的数据；蓝色的点是从均值为 3.0、方差为 4.0 的高斯分布中采样得到的数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cd086-bcbe-47d5-839a-6b139384a279",
   "metadata": {},
   "source": [
    "混合高斯分布必须在不知道每个数据点的类别的情况下求出参\n",
    "数。因此，我们需要在推测“每个数据点属于某个类别”的权重的基础上，计算出数据点的各个类\n",
    "别的高斯分布的参数（均值和方差）。</br>\n",
    "详细分为以下步骤\n",
    "1. 初始化参数（各高斯分布的均值和方差）。\n",
    "2. 对每个类别计算数据点的权重。\n",
    "3. 根据步骤 2 中计算出的权重重新计算参数。\n",
    "4. 重复步骤 2 和步骤 3，直到通过步骤 3 更新前后的每个均值的变化足够小。</br>\n",
    "![title](./image/gaussprocess.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6de66-6a12-485f-9ebd-b5e43b78a0a2",
   "metadata": {},
   "source": [
    "下面是对包含 3 个品种的鸢尾花数据应用混合高斯分布的示例代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c14945de-e9db-4e61-a295-c4870e934170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 0 2\n",
      " 2 2 2 0 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "[[6.54639415 2.94946365 5.48364578 1.98726565]\n",
      " [5.006      3.428      1.462      0.246     ]\n",
      " [5.9170732  2.77804839 4.20540364 1.29848217]]\n",
      "[[[0.38744093 0.09223276 0.30244302 0.06087397]\n",
      "  [0.09223276 0.11040914 0.08385112 0.05574334]\n",
      "  [0.30244302 0.08385112 0.32589574 0.07276776]\n",
      "  [0.06087397 0.05574334 0.07276776 0.08484505]]\n",
      "\n",
      " [[0.121765   0.097232   0.016028   0.010124  ]\n",
      "  [0.097232   0.140817   0.011464   0.009112  ]\n",
      "  [0.016028   0.011464   0.029557   0.005948  ]\n",
      "  [0.010124   0.009112   0.005948   0.010885  ]]\n",
      "\n",
      " [[0.2755171  0.09662295 0.18547072 0.05478901]\n",
      "  [0.09662295 0.09255152 0.09103431 0.04299899]\n",
      "  [0.18547072 0.09103431 0.20235849 0.06171383]\n",
      "  [0.05478901 0.04299899 0.06171383 0.03233775]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.mixture import GaussianMixture\n",
    "data = load_iris()\n",
    "n_components = 3\n",
    "# 高斯分布的数量\n",
    "model = GaussianMixture(n_components=n_components)\n",
    "model.fit(data.data)\n",
    "print(model.predict(data.data))\n",
    "print(model.means_)\n",
    "# 预测类别\n",
    "# 各高斯分布的均值\n",
    "print(model.covariances_)\n",
    "# 各高斯分布的方差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47553c9-a5a1-4e06-be9c-0c7976a5e75d",
   "metadata": {},
   "source": [
    "### 3.7 LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68252769-6f53-47c0-a4b2-21c82f9d9396",
   "metadata": {},
   "source": [
    "LLE 是一种被称为流形学习（manifold learning）的算法，它的目标是对具有非线性结构的数据\n",
    "进行降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f18925-0d66-4b87-a589-3ba027b04c64",
   "metadata": {},
   "source": [
    "算法说明</br>\n",
    "LLE 算法要求数据点由其近邻点的线性组合来表示。我们用一个例子来解释</br>\n",
    "对于数据点 x1，以最接近 x1 的两个点 x2 和 x3 的线性组合来表示它。</br>\n",
    "当 x1 = (1, 1, 1)，x2 = ( - 1, 0, - 1)，\n",
    "x3 = (2, 3, 2) 时 的 情 况， 如 果 x2 和 x3 的 权 重 分 别 为 w12 = - 1/3，w13 = 1/3， 那 么 x1 可 以 表 示 为\n",
    "x1 = w12x2 + w13x3。</br>如果在不扭转三维空间的 3 个点之间的关系的前提下把它们移动到二维空间，我\n",
    "们就可以继续使用这些权重来以近邻点表示 x1。</br>\n",
    "实际的数据虽然在三维空间上呈现瑞士卷那样的卷曲结构，但 LLE 将其视为局部的点与点的\n",
    "关系，即近邻点之间是不弯曲的空间，所以仍使用近邻点来表示数据点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6f3b4-8cf2-4654-b9a2-07fc9f8b8590",
   "metadata": {},
   "source": [
    "![title](./image/LLE.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0de2a0-d2f1-474a-adcc-eadb8ccdde6a",
   "metadata": {},
   "source": [
    "下面是对瑞士卷数据集应用 LLE 的代码，其中设置的近邻点的数量为 12。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6e83554-9293-49a6-a43d-04b358f9a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00745799  0.01458373]\n",
      " [-0.00644212  0.00619955]\n",
      " [-0.01741644 -0.03293017]\n",
      " ...\n",
      " [ 0.03863832 -0.0091499 ]\n",
      " [ 0.02496744 -0.01135749]\n",
      " [-0.0015111  -0.00145397]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "data, color = make_swiss_roll(n_samples=1500)\n",
    "n_neighbors = 12\n",
    "n_components = 2\n",
    "# 近邻点的数量\n",
    "# 降维后的维度\n",
    "model = LocallyLinearEmbedding(n_neighbors=n_neighbors,\n",
    "n_components=n_components)\n",
    "model.fit(data)\n",
    "print(model.transform(data)) # 变换后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c20d6e-5787-4bb5-88c4-4a028ab61c0f",
   "metadata": {},
   "source": [
    "### 3.8 t -SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7db991-72ce-468e-b0a1-a5c14a266a28",
   "metadata": {},
   "source": [
    "t-SNE 是一种流形学习算法，用于实现复杂数据的可视化。我们可以通过将高维数据降维为二\n",
    "维或三维来实现可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fa597-f842-4b72-b870-cd307436a39d",
   "metadata": {},
   "source": [
    "下面是将 t-SNE 应用于手写数字数据集的代码，其中设置的降维后的维度是 2。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63544ab0-8350-4cc3-8679-71d90cf46408",
   "metadata": {},
   "source": [
    "![title](./image/jiangwei.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e3883-862f-40a9-b6b2-bb59762b3778",
   "metadata": {},
   "source": [
    "算法说明\n",
    "1. 对于所有的组 i、j，使用高斯分布来表示 xi 和 xj 的相似度。\n",
    "2. 在低维空间中随机配置与 x i 相同数量的点 y i，对于所有的组 i、j，使用 t 分布表示 y i 和 y j 的\n",
    "相似度。\n",
    "3. 更新数据点 yi，使得步骤 1 和步骤 2 中定义的相似度分布尽可能相似。\n",
    "4. 重复步骤 3，直到达到收敛条件。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec4ee72-952b-436e-84ca-75152012882c",
   "metadata": {},
   "source": [
    "![title](./image/SNEprocess.jpg)</br>\n",
    "上图中，横轴是距离，纵轴是相似度。从图中可以看出，数据之间的距离越近，相似度\n",
    "越高；距离越远，相似度越低。我们首先在原来的高维空间中用高斯分布计算相似度，以 pij 这个分\n",
    "布表示。这个 pij 表示数据点 xi 和 xj 之间的相似度。\n",
    "接下来，在低维空间中随机配置与 x i 对应的数据点 y i。我们也对这个数据点计算表示相似度的\n",
    "qij，不过这时使用的是 t 分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac945bbe-56e4-44b4-89ef-fbed2dbdbf76",
   "metadata": {},
   "source": [
    "![title](./image/tfenbu.jpg)</br>\n",
    "在计算出 pij 和 qij 之后，我们来更新数据点 yi，使 qij 具有与 pij 相同的分布，这样就能够以低维\n",
    "空间的 y i 再现高维空间中各 x i 的相似度的关系。由于这时在低维空间使用的是 t 分布，所以可以看\n",
    "到，当再现大的相似度时，数据点在低维空间中配置的距离更近；反之，当再现小的相似度时，数\n",
    "据点在低维空间中配置的距离更远。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9084604f-9f70-4ca0-8869-5a5774c4a871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.016503   54.184124 ]\n",
      " [ 10.382164  -11.731034 ]\n",
      " [-13.537349  -18.758026 ]\n",
      " ...\n",
      " [ -5.8934097 -10.716763 ]\n",
      " [-21.116165   15.562969 ]\n",
      " [-14.097248   -6.917012 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "n_components = 2\n",
    "# 设置降维后的维度为2\n",
    "model = TSNE(n_components=n_components)\n",
    "print(model.fit_transform(data.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
