


%matplotlib inline
import matplotlib.pyplot as plt





from sklearn.linear_model import LinearRegression
X = [[10.0], [8.0], [13.0], [9.0], [11.0], [14.0],
 [6.0], [4.0], [12.0], [7.0], [5.0]]
y = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96,
 7.24, 4.26, 10.84, 4.82, 5.68]
model = LinearRegression()
model.fit(X, y)
print(model.intercept_) # 截距
print(model.coef_) # 斜率
y_pred = model.predict([[0], [1]])
print(y_pred) # 对x=0, x=1的预测结果


# 绘制数据集散点图
plt.scatter(X, y, color='blue', label='Data')
# 绘制回归线
y_pred = model.predict(X)
plt.plot(X, y_pred, color='red', label='Linear Regression Model')

















import numpy as np
from sklearn.preprocessing import PolynomialFeatures #用于生成多项式特征
from sklearn.linear_model import Ridge # 实现岭回归模型
from sklearn.metrics import mean_squared_error # 用于计算均方误差
train_size = 20
test_size = 12
# 随机生成训练集和测试集
train_X = np.random.uniform(low=0, high=1.2, size=train_size)
test_X = np.random.uniform(low=0.1, high=1.3, size=test_size)
train_y = np.sin(train_X * 2 * np.pi) + np.random.normal(0, 0.2, train_size)
test_y = np.sin(test_X * 2 * np.pi) + np.random.normal(0, 0.2, test_size)
# 多项式次数为6
poly = PolynomialFeatures(6)
train_poly_X = poly.fit_transform(train_X.reshape(train_size, 1))
test_poly_X = poly.fit_transform(test_X.reshape(test_size, 1))
# 设置正则化参数为1.0
model = Ridge(alpha=1.0)
model.fit(train_poly_X, train_y)
train_pred_y = model.predict(train_poly_X)
test_pred_y = model.predict(test_poly_X)
print(mean_squared_error(train_pred_y, train_y))
print(mean_squared_error(test_pred_y, test_y))


# 绘制拟合曲线
x_values = np.linspace(0, 1.5, 100).reshape(-1, 1)
x_poly = poly.transform(x_values)
y_values = model.predict(x_poly)

plt.scatter(train_X, train_y, color='blue', label='Train Data')
plt.scatter(test_X, test_y, color='red', label='Test Data')
plt.plot(x_values, y_values, color='green', label='Fitted Curve')
plt.xlabel('X')
plt.ylabel('y')























import numpy as np
from sklearn.linear_model import LogisticRegression
# 生成均值为3标准差为1的正态分布数据和均值为-1标准差为1的正态分布数据，并将它们合并为一个大小为(100,1)的二维数组
X_train = np.r_[np.random.normal(3, 1, size=50),
np.random.normal(-1, 1, size=50)].reshape((100, -1))
# 生成50个值全为1和50个值全为0的数组
y_train = np.r_[np.ones(50), np.zeros(50)]
model = LogisticRegression()
model.fit(X_train, y_train)
model.predict_proba([[0], [1], [2]])[:, 1]


plt.scatter(X_train, y_train, c=y_train, cmap='viridis', edgecolors='k')
plt.xlabel('Feature')
plt.ylabel('Target')
x_values = np.linspace(-3, 6, 300).reshape(-1, 1)
y_values = model.predict_proba(x_values)[:, 1]
plt.plot(x_values, y_values, color='red', label='Logistic Regression Model')














from sklearn.svm import LinearSVC
from sklearn.datasets import make_blobs #用于生成聚类数据集
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# 生成数据，定义两个聚类中心的坐标，用于生成聚类数据集
centers = [(-1, -0.125), (0.5, 0.5)]
X, y = make_blobs(n_samples=50, n_features=2,
centers=centers, cluster_std=0.3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
model = LinearSVC()
model.fit(X_train, y_train)
# 训练
y_pred = model.predict(X_test)
accuracy_score(y_pred, y_test)
# 评估


plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', edgecolors='k')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
# 获取决策边界
w = model.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-2, 2)
yy = a * xx - (model.intercept_[0]) / w[1]
# 绘制决策边界
plt.plot(xx, yy, 'k-')




















from sklearn.svm import SVC
from sklearn.datasets import make_gaussian_quantiles
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# 生成一个包含300个样本、2个特征的高斯量化数据集，其中指定了两个类别和两个特征
X, y = make_gaussian_quantiles(n_features=2, n_classes=2, n_samples=300)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
model = SVC()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy_score(y_pred, y_test)


x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.5)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')





























from sklearn.naive_bayes import MultinomialNB
# 生成数据
X_train = [[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],
           [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
           [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],
           [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],
           [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],
           [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1]]
y_train = [1, 1, 1, 0, 0, 0]
model = MultinomialNB()
model.fit(X_train, y_train)
# 训练
model.predict([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]])
# 评估
































from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# 读取数据
data = load_wine()
X_train, X_test, y_train, y_test = train_test_split(
data.data, data.target, test_size=0.3)
model = RandomForestClassifier()
model.fit(X_train, y_train)
# 训练
y_pred = model.predict(X_test)
accuracy_score(y_pred, y_test)
# 评估


























from sklearn.datasets import load_digits
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# 读取数据
data = load_digits()
X = data.images.reshape(len(data.images), -1)
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
model = model = MLPClassifier(hidden_layer_sizes=(16, ))
model.fit(X_train, y_train)
# 训练
y_pred = model.predict(X_test)
accuracy_score(y_pred, y_test)
# 评估














from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# 生成数据
X, y = make_moons(noise=0.3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# 最近邻点k的数量采用默认值5
model = KNeighborsClassifier()
model.fit(X_train, y_train)
# 训练
y_pred = model.predict(X_test)
accuracy_score(y_pred, y_test)
# 评估


# 绘制决策边界
h = .02  # 步长
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.5)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')






